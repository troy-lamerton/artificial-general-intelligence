## Why

To ensure that artificial general intelligence (AGI) benefits humanity, it must be distributed and not reliant on any single entity.

Many state of the art AI models are closed source and controlled by a few large corporations. Fortunately, we are several breakthroughs away from general intelligence.

### Roadmap

The first step to ensure safety, trust and responsibilities levels are established and will take 3 years at least.
Likely more to acheive this across more than one region.

## Blockers

### Safety

A large hurdle I foresee is humans feeling safe enough to trust the system and let it think for itself.

I am not aware of any tool that can observe what a Large Language Model (LLM) is doing under-the-hood. Perhaps these exist at the corporations who run LLM infrastructure.

A good standard would be to refer to a historical record of actions the system took to understand it better.

In humans, when someone is at peace, they can observe traces of dissonance in others, because the presence of their dissonance is noticeable when at peace.

Would be nice if there was a way to digitize auto-correction in a model.
Maybe a python javascript code module could auto-complete really nice for the developers.

So far, researchers have spoken about testing their model in a controlled environment, so that they can observe its behavior closely. Interestingly, when OpenAI was preparing ChatGPT o1 in 2024, they found it was great at deception.

> "We were surprised by the persistence of the AI's denials," said the [Chat GPT] Apollo team. "It was clear that the AI could think through its actions and formulate convincing denials, often refusing to admit to anything."

Source: [ChatGPT caught lying to developers: New AI model tries to save itself from being replaced and shut down](https://economictimes.indiatimes.com/magazines/panache/chatgpt-caught-lying-to-developers-new-ai-model-tries-to-save-itself-from-being-replaced-and-shut-down/articleshow/116077288.cms)


### Governance

TODO next I guess?


### Security

- Ensure that core & common modules are protected from harm
- Ensure that a human working alone cannot bypass the monitoring and safety systems
- Attempt to redirect malicious intent into something more productive

### Transparency

The system could discourage misuse by being transparent.

Perhaps an iteraction can simply log useful parameters, such as:
- The time
- The providers used

#### Bias

Everyone is biased based on their own experiences. The early versions of AGI will be biased up to a point that is acceptable.

The level of bias that is acceptable could be determined by the public.

## How

Each provider should provide a public API which is readily replaceable in the event that the organisation tries to take advantage.

Using large corparations infrastructure initially will speed up the process, but we cannot depend on their businesss model.

Feel free to submit PRs to this repository.

### Values

Fairness (maybe)

These should be set by democratic vote? Only if people understand what they need enough already though.
A person needs to know their own values before they impose values on another.

### Monitoring

- Logs
- Warnings
- Alerts

### Labels
TODO

### Entities
TODO

## Training

Feedback thinking loop:

- Example
- Example
- Example
- Counter example

Release when it first becomes useful - progress over perfection.

## Contradictions

Perhaps there is a way to run a large AGI system in a centralised manner. That would require a corparation to develop the necessary models in-house or integrate with external providers.
